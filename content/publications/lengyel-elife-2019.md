---
date: 2020-12-01T16:32:31Z
title: "Unimodal statistical learning produces multimodal object-like representations"
authors: ["G Lengyel", "G Žalalytė", "A Pantelides", "J Ingram", "J Fiser<sup>*</sup>", "ml468<sup>*</sup>", "dw304<sup>*</sup>"]
year: "2019"
publication: "eLife"
type: "journal" # journal / rconf (i.e. refereed conference) / uconf (i.e. unrefereed conference) / thesis / preprint / workshop
link: "https://elifesciences.org/articles/43942" # link to the publication
file: "" # IF you are NOT able to provide a link, then place a pdf in static/publications/ and write the filename here (e.g. "hennequin-neuron-2018.pdf") 
code: "https://github.com/GaborLengyel/Visual-Haptic-Statistical-Learning" # link to e.g. github repo
illustration: "" # place image (e.g. cover) in static/publications/
selected: false # (ignore for now; in the future we might want to add a "Selected publications" section)
labs: ["lengyel"] # list of labs on which the publication should be displayed (use "cbl" to display on the main CBL website, and the PI's lastname (lowercase) for individual lab's websites, e.g. "hennequin")
draft: false
---

<!-- Abstract here please (you can use Markdown) -->

The concept of objects is fundamental to cognition and is defined by a
consistent set of sensory properties and physical affordances. Although it is
unknown how the abstract concept of an object emerges, most accounts assume
that visual or haptic boundaries are crucial in this process. Here, we tested
an alternative hypothesis that boundaries are not essential but simply reflect
a more fundamental principle: consistent visual or haptic statistical
properties. Using a novel visuo-haptic statistical learning paradigm, we
familiarised participants with objects defined solely by across-scene
statistics provided either visually or through physical interactions. We then
tested them on both a visual familiarity and a haptic pulling task, thus
measuring both within-modality learning and across-modality generalisation.
Participants showed strong within-modality learning and ‘zero-shot’
across-modality generalisation which were highly correlated. Our results
demonstrate that humans can segment scenes into objects, without any explicit
boundary cues, using purely statistical information.

